# -*- coding: utf-8 -*-
"""autolysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_zcnLhFANLM-YgQAeGXC5D0FIRhCPZih
"""

import os
import sys
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import chardet
import json
import httpx

# Set AI Proxy Token from Environment Variable
AIPROXY_TOKEN = os.getenv("AIPROXY_TOKEN")
if not AIPROXY_TOKEN:
    print("Error: AIPROXY_TOKEN environment variable not set.")
    sys.exit(1)

# Function to send data to LLM
def ask_llm(prompt, additional_context=None):
    url = "https://aiproxy.sanand.workers.dev/v1/chat/completions"
    headers = {"Authorization": f"Bearer {AIPROXY_TOKEN}"}
    payload = {
        "model": "gpt-4o-mini",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
    }
    if additional_context:
        payload["messages"].append({"role": "user", "content": additional_context})
    response = httpx.post(url, headers=headers, json=payload)
    return response.json()["choices"][0]["message"]["content"]

# Function to detect file encoding
def detect_encoding(file_path):
    with open(file_path, 'rb') as f:
        raw_data = f.read(1024)  # Read a small part of the file
    result = chardet.detect(raw_data)
    detected_encoding = result['encoding']

    # Handle cases where encoding is incorrectly detected as 'ascii'
    if detected_encoding in [None, 'ascii']:
        detected_encoding = 'utf-8-sig'  # Fallback to a broader UTF variant
    return detected_encoding

# Function to create README.md
def create_readme(content, readme_path):
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(content)

# Function to generate a heatmap for numerical data
def generate_heatmap(df, heatmap_path):
    corr_matrix = df.corr()  # Correlation matrix of numerical columns
    plt.figure(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
    plt.title("Correlation Heatmap")
    plt.tight_layout()
    plt.savefig(heatmap_path)
    plt.close()

# Function to generate bar plots for categorical variables
def generate_category_plots(df, category_columns, output_dir):
    for column in category_columns:
        plt.figure(figsize=(8, 6))
        sns.countplot(data=df, x=column, palette="Set2")
        plt.title(f"Distribution of {column}")
        plt.xticks(rotation=45, ha="right")
        plt.tight_layout()
        plt.savefig(f"{output_dir}/{column}_category_plot.png")
        plt.close()

# Main analysis pipeline
def analyze_dataset(file_path):
    try:
        # Step 1: Detect encoding and load dataset
        encoding = detect_encoding(file_path)
        print(f"Detected file encoding: {encoding}")

        try:
            df = pd.read_csv(file_path, encoding=encoding)
        except UnicodeDecodeError:
            # If the detected encoding fails, fallback to a common encoding
            print(f"Failed to decode with encoding: {encoding}. Falling back to 'latin1'.")
            df = pd.read_csv(file_path, encoding='latin1')

        dataset_name = os.path.splitext(os.path.basename(file_path))[0]

        # Step 2: Basic Analysis
        summary = df.describe(include="all").to_string()
        missing_values = df.isnull().sum()
        num_missing = missing_values[missing_values > 0]
        print(f"Dataset Summary:\n{summary}\n")
        print(f"Missing Values:\n{num_missing}\n")

        # Step 3: Handle categorical columns
        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()
        print(f"Categorical columns: {categorical_columns}")

        # Generate bar plots for categorical variables
        if categorical_columns:
            generate_category_plots(df, categorical_columns, dataset_name)

        # Step 4: Correlation and Visualization for numerical data
        heatmap_path = None
        if not df.select_dtypes(include=['number']).empty:
            heatmap_path = f"{dataset_name}/heatmap.png"
            generate_heatmap(df, heatmap_path)

        # Step 5: Advanced Analysis using LLM
        column_info = df.dtypes.apply(str).to_dict()
        example_values = df.head(3).to_dict(orient="records")
        analysis_prompt = (
            f"The dataset has {len(df)} rows and {len(df.columns)} columns.\n"
            f"Columns:\n{json.dumps(column_info, indent=2)}\n"
            f"Example rows:\n{json.dumps(example_values, indent=2)}\n"
            "Provide insights, key patterns, and potential anomalies."
        )
        analysis = ask_llm(analysis_prompt)

        # Step 6: Generate Narrative
        narrative_prompt = (
            f"Based on the analysis:\n{analysis}\n"
            f"Write a Markdown report summarizing the dataset, insights, and implications. "
            f"Include the heatmap {heatmap_path if heatmap_path else '(no heatmap generated)'} if appropriate."
        )
        narrative = ask_llm(narrative_prompt)

        # Step 7: Save Results
        output_dir = f"{dataset_name}"
        os.makedirs(output_dir, exist_ok=True)

        # Create and save README.md in the corresponding dataset folder
        readme_path = os.path.join(output_dir, "README.md")
        create_readme(narrative, readme_path)

        # Save heatmap image in the dataset folder
        if heatmap_path:
            os.rename(heatmap_path, os.path.join(output_dir, "heatmap.png"))

        print(f"Analysis complete. Results saved to {readme_path}.")

    except Exception as e:
        print(f"Error during analysis: {e}")
        sys.exit(1)

if __name__ == "__main__":
    dataset_file = sys.argv[1]
    analyze_dataset(dataset_file)

